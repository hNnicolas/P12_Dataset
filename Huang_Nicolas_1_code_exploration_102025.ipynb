{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Projet ZenAssist – Classification des réclamations clients\n",
       "\n",
       "### Objectifs :\n",
       "1. Développer un prompt pour prédire l’étiquette d’une réclamation avec un LLM\n",
       "2. Tester le modèle sur 20 exemples (jeu d’entraînement)\n",
       "3. Essayer plusieurs variantes de prompt pour améliorer la qualité\n",
       "4. Évaluer sur ~1000 exemples du jeu de test et calculer les métriques\n",
       "5. Comparer les performances avec des modèles ML classiques (TF-IDF + ML)\n"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "!pip install mistralai scikit-learn pandas matplotlib"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Import des bibliothèques"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "import time\n",
       "import pandas as pd\n",
       "import matplotlib.pyplot as plt\n",
       "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
       "from sklearn.model_selection import train_test_split\n",
       "from mistralai.client import MistralClient\n",
       "from mistralai.models.chat_completion import ChatMessage\n",
       "\n",
       "pd.set_option('display.max_columns', None)\n",
       "plt.style.use('seaborn')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Initialisation du client Mistral"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "# Remplacer par votre clé API\n",
       "API_KEY = \"YOUR_API_KEY_HERE\"\n",
       "client = MistralClient(api_key=API_KEY)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Chargement des données\n",
       "- Ici, on simule le dataset. Remplacer par le vrai CSV si disponible."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "data = {\n",
       "    \"reclamation\": [\n",
       "        \"Mon produit est arrivé cassé.\",\n",
       "        \"Impossible de me connecter à mon compte.\",\n",
       "        \"Le paiement a été refusé alors que ma carte est valide.\",\n",
       "        \"La livraison a pris 3 semaines au lieu de 3 jours.\",\n",
       "        \"Mon mot de passe ne fonctionne plus.\",\n",
       "        \"Le colis ne correspond pas à ma commande.\",\n",
       "        \"Je veux annuler mon abonnement.\",\n",
       "        \"Le produit ne démarre pas après l’installation.\",\n",
       "        \"J’ai été débité deux fois pour la même commande.\",\n",
       "        \"La taille du vêtement n’est pas conforme au guide.\"\n",
       "    ] * 200,\n",
       "    \"etiquette\": [\n",
       "        \"Produit\", \"Compte\", \"Paiement\", \"Livraison\", \"Compte\",\n",
       "        \"Produit\", \"Abonnement\", \"Produit\", \"Paiement\", \"Produit\"\n",
       "    ] * 200\n",
       "}\n",
       "df = pd.DataFrame(data)\n",
       "\n",
       "# Split train/test\n",
       "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['etiquette'])\n",
       "print(f\"Taille dataset : {len(df)}, Train : {len(train_df)}, Test : {len(test_df)}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Définition des prompts (3 variantes à tester)"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def build_prompt_v1(text):\n",
       "    return f\"\"\"\n",
       "Tu es un classificateur de réclamations clients.\n",
       "Classe la réclamation suivante dans UNE SEULE catégorie parmi :\n",
       "- Produit, Compte, Paiement, Livraison, Abonnement\n",
       "\n",
       "Réclamation : \\\"{text}\\\"\n",
       "Réponds uniquement avec la catégorie.\n",
       "\"\"\"\n",
       "\n",
       "def build_prompt_v2(text):\n",
       "    return f\"\"\"\n",
       "Analyse la réclamation suivante et donne uniquement l’étiquette correspondante.\n",
       "Choix possibles : Produit, Compte, Paiement, Livraison, Abonnement.\n",
       "\n",
       "Réclamation : {text}\n",
       "Réponse attendue : une seule étiquette.\n",
       "\"\"\"\n",
       "\n",
       "def build_prompt_v3(text):\n",
       "    return f\"\"\"\n",
       "Catégorise la réclamation suivante dans le domaine concerné.\n",
       "Options : Produit / Compte / Paiement / Livraison / Abonnement.\n",
       "\n",
       "Réclamation : {text}\n",
       "Réponds uniquement par le nom exact de l’option.\n",
       "\"\"\"\n",
       "\n",
       "prompts = {\n",
       "    'Prompt v1': build_prompt_v1,\n",
       "    'Prompt v2': build_prompt_v2,\n",
       "    'Prompt v3': build_prompt_v3,\n",
       "}"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Fonction d’appel LLM"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def classify_with_llm(text, build_prompt, model=\"open-mixtral-8x7b\", temperature=0.0):\n",
       "    \"\"\"Classe une réclamation en utilisant un prompt spécifique et le modèle Mistral.\"\"\"\n",
       "    response = client.chat(\n",
       "        model=model,\n",
       "        messages=[ChatMessage(role=\"user\", content=build_prompt(text))],\n",
       "        temperature=temperature\n",
       "    )\n",
       "    return response.choices[0].message.content.strip()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Test rapide sur 20 exemples (jeu d’entraînement)"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "sample_df = train_df.sample(20, random_state=42)\n",
       "\n",
       "for prompt_name, builder in prompts.items():\n",
       "    print(f\"\\n=== {prompt_name} (20 exemples) ===\")\n",
       "    preds = [classify_with_llm(txt, builder) for txt in sample_df['reclamation']]\n",
       "    print(classification_report(sample_df['etiquette'], preds))"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 7. Évaluation sur ~1000 exemples du jeu de test"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "subset_df = test_df.sample(min(1000, len(test_df)), random_state=42)\n",
       "results = {}\n",
       "\n",
       "for prompt_name, builder in prompts.items():\n",
       "    print(f\"\\n=== {prompt_name} (1000 exemples) ===\")\n",
       "    start_time = time.time()\n",
       "    preds = [classify_with_llm(txt, builder) for txt in subset_df['reclamation']]\n",
       "    end_time = time.time()\n",
       "\n",
       "    acc = accuracy_score(subset_df['etiquette'], preds)\n",
       "    f1 = f1_score(subset_df['etiquette'], preds, average='weighted')\n",
       "    avg_time = (end_time - start_time)/len(subset_df)\n",
       "\n",
       "    print(f\"Accuracy: {acc:.2f}\")\n",
       "    print(f\"F1-score: {f1:.2f}\")\n",
       "    print(f\"Temps moyen de réponse: {avg_time:.2f} sec\")\n",
       "    print(classification_report(subset_df['etiquette'], preds))\n",
       "\n",
       "    results[prompt_name] = {'accuracy': acc, 'f1': f1, 'time': avg_time}\n",
       "\n",
       "    cm = confusion_matrix(subset_df['etiquette'], preds, labels=subset_df['etiquette'].unique())\n",
       "    disp = ConfusionMatrixDisplay(cm, display_labels=subset_df['etiquette'].unique())\n",
       "    disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)\n",
       "    plt.title(f\"Matrice de confusion - {prompt_name}\")\n",
       "    plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 8. Implémentation ML traditionnelle (TF-IDF + modèles ML)"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "from sklearn.feature_extraction.text import TfidfVectorizer\n",
       "from sklearn.preprocessing import LabelEncoder\n",
       "from sklearn.naive_bayes import MultinomialNB\n",
       "from sklearn.linear_model import LogisticRegression\n",
       "from sklearn.svm import LinearSVC\n",
       "\n",
       "# Vectorisation TF-IDF\n",
       "vectorizer = TfidfVectorizer(stop_words='french', max_features=5000)\n",
       "X_train_ml = vectorizer.fit_transform(train_df['reclamation'])\n",
       "X_test_ml = vectorizer.transform(test_df['reclamation'])\n",
       "\n",
       "# Encodage des étiquettes\n",
       "label_encoder = LabelEncoder()\n",
       "y_train_ml = label_encoder.fit_transform(train_df['etiquette'])\n",
       "y_test_ml = label_encoder.transform(test_df['etiquette'])\n",
       "\n",
       "# Modèles ML\n",
       "ml_models = {\n",
       "    'MultinomialNB': MultinomialNB(),\n",
       "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
       "    'LinearSVC': LinearSVC()\n",
       "}\n",
       "ml_results = {}\n",
       "\n",
       "for name, model in ml_models.items():\n",
       "    print(f\"\\n=== {name} ===\")\n",
       "    start_time = time.time()\n",
       "    model.fit(X_train_ml, y_train_ml)\n",
       "    train_time = time.time() - start_time\n",
       "\n",
       "    start_time = time.time()\n",
       "    y_pred = model.predict(X_test_ml)\n",
       "    test_time = time.time() - start_time\n",
       "\n",
       "    acc = accuracy_score(y_test_ml, y_pred)\n",
       "    f1 = f1_score(y_test_ml, y_pred, average='weighted')\n",
       "\n",
       "    print(f\"Accuracy: {acc:.2f}, F1-score: {f1:.2f}\")\n",
       "    print(f\"Temps entraînement: {train_time:.2f}s, Prédiction: {test_time:.2f}s\")\n",
       "    print(classification_report(y_test_ml, y_pred, target_names=label_encoder.classes_))\n",
       "\n",
       "    cm = confusion_matrix(y_test_ml, y_pred)\n",
       "    disp = ConfusionMatrixDisplay(cm, display_labels=label_encoder.classes_)\n",
       "    disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)\n",
       "    plt.title(f\"Matrice de confusion - {name}\")\n",
       "    plt.show()\n",
       "\n",
       "    ml_results[name] = {'accuracy': acc, 'f1': f1, 'train_time': train_time, 'pred_time': test_time}"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 9. Comparaison LLM vs ML"
      ]
     },
     {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "llm_metrics_df = pd.DataFrame(results).T[['accuracy','f1']].rename(columns={'accuracy':'Accuracy LLM','f1':'F1 LLM'})",
          "ml_metrics_df = pd.DataFrame(ml_results).T[['accuracy','f1']].rename(columns={'accuracy':'Accuracy ML','f1':'F1 ML'})",
          "",
          "comparison_df = pd.concat([llm_metrics_df, ml_metrics_df], axis=1, join='inner')",
          "print(\"\\n=== Comparaison LLM vs ML ===\")",
          "print(comparison_df)",
          "",
          "comparison_df.plot(kind='bar', figsize=(10,6))",
          "plt.title(\"Performance comparée : LLM vs ML\")",
          "plt.ylabel(\"Score\")",
          "plt.xticks(rotation=45)",
          "plt.show()"
        ]
      }
    ],
    "metadata": {
      "kernelspec": {
        "display_name": "Python 3",
        "language": "python",
        "name": "python3"
      },
      "language_info": {
        "name": "python",
        "version": "3.11"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 5
  }